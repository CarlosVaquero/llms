{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy \n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "# import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    " \n",
    "from tqdm import tqdm\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_excel('LLM_examples.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MH15UQ/conda3/envs/llms/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "# MODEL = \"DTAI-KULeuven/robbertje-merged-dutch-sentiment\"\n",
    "\n",
    "MODEL = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_data(df_train, df_test, model_name, label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}):\n",
    "\n",
    "    # Load datasets\n",
    "    dataset_train = Dataset.from_pandas(df_train)\n",
    "    dataset_test = Dataset.from_pandas(df_test)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)#(\"google-bert/bert-base-cased\")\n",
    " \n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"Sentence\"], padding=\"max_length\", truncation=True)\n",
    " \n",
    "    # Map sentiment labels to integers\n",
    "\n",
    "    #label_mapping = {'Negatief': 0, 'Neutraal': 1, 'Positief': 2}\n",
    " \n",
    "    def map_labels(examples):\n",
    "        examples['labels'] = [label_mapping[label] for label in examples['Sentiment']]\n",
    "        return examples\n",
    " \n",
    "    # Tokenize datasets\n",
    "    small_train_dataset = dataset_train.map(tokenize_function, batched=True)\n",
    "    small_eval_dataset = dataset_test.map(tokenize_function, batched=True)\n",
    " \n",
    "    # Apply label mapping\n",
    "    small_train_dataset = small_train_dataset.map(map_labels, batched=True)\n",
    "    small_eval_dataset = small_eval_dataset.map(map_labels, batched=True)\n",
    "\n",
    "    return small_train_dataset, small_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_llm(df_train, df_test, model_name, num_labels, label_mapping = {'Negatief': 0, 'Neutraal': 1, 'Positief': 2}, epochs=3):\n",
    " \n",
    "    small_train_dataset, small_eval_dataset = (\n",
    "        tokenizer_data(df_train=df_train, \n",
    "                       df_test=df_test, \n",
    "                       model_name = model_name,\n",
    "                       label_mapping = label_mapping)\n",
    "    )\n",
    " \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "\n",
    "    # Load metric\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    " \n",
    "    # Compute metrics function\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    " \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        run_name=\"my_unique_run_name\",\n",
    "        report_to=[],  # Disable W&B logging\n",
    "        logging_dir='./logs',  # Directory for storing logs\n",
    "        logging_steps=10,  # Log every 10 steps\n",
    "        eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "        save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "        per_device_train_batch_size=4,  # Adjust based on your setup\n",
    "        per_device_eval_batch_size=4,  # Adjust based on your setup\n",
    "        num_train_epochs=epochs,  # Number of training epochs\n",
    "        load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    )\n",
    " \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_train_dataset,\n",
    "        eval_dataset=small_eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    " \n",
    "    # Disable W&B logging\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    " \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer, small_train_dataset, small_eval_dataset    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e588003d4c104a76bb875a352a8d83ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6640e0e73ad74f248dbd984dbc760765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef17235a94e482eb1dabb131d1881c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a811cf7d1d8c491481e8599b5ebbe55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_train_dataset, small_eval_dataset = (\n",
    "    tokenizer_data(df_train=sentences, \n",
    "                    df_test=sentences, \n",
    "                    model_name = MODEL,\n",
    "                    label_mapping = {'Negatief': 0, 'Neutraal': 1, 'Positief': 2})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdb9b7b819542a19fbffd4e49bfeda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3129684dd80e4f2faf853d3f8b754472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715327642d3147a7923f8e2d75324524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677014fdb5464b1db3ae9c7b8bcee49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-v2-dutch-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92c761e5d7a4f7ca1c0c9900fb790bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0512, 'grad_norm': 9.979782104492188, 'learning_rate': 4.9335989375830016e-05, 'epoch': 0.04}\n",
      "{'loss': 0.809, 'grad_norm': 24.539335250854492, 'learning_rate': 4.867197875166003e-05, 'epoch': 0.08}\n",
      "{'loss': 0.3882, 'grad_norm': 2.535130023956299, 'learning_rate': 4.8007968127490044e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4383, 'grad_norm': 1.3243513107299805, 'learning_rate': 4.734395750332006e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4687, 'grad_norm': 54.80116653442383, 'learning_rate': 4.6679946879150064e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4039, 'grad_norm': 0.14871855080127716, 'learning_rate': 4.601593625498008e-05, 'epoch': 0.24}\n",
      "{'loss': 0.7554, 'grad_norm': 57.78717041015625, 'learning_rate': 4.535192563081009e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3846, 'grad_norm': 6.090554714202881, 'learning_rate': 4.4687915006640105e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2683, 'grad_norm': 5.018764019012451, 'learning_rate': 4.402390438247012e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2508, 'grad_norm': 0.12534798681735992, 'learning_rate': 4.335989375830013e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1197, 'grad_norm': 0.058680638670921326, 'learning_rate': 4.2695883134130146e-05, 'epoch': 0.44}\n",
      "{'loss': 0.5818, 'grad_norm': 100.19976806640625, 'learning_rate': 4.203187250996016e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0654, 'grad_norm': 0.04273100197315216, 'learning_rate': 4.1367861885790174e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5457, 'grad_norm': 0.13252870738506317, 'learning_rate': 4.070385126162019e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5417, 'grad_norm': 0.04598497971892357, 'learning_rate': 4.00398406374502e-05, 'epoch': 0.6}\n",
      "{'loss': 0.1687, 'grad_norm': 0.050480421632528305, 'learning_rate': 3.9375830013280215e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2302, 'grad_norm': 0.10953385382890701, 'learning_rate': 3.871181938911023e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1688, 'grad_norm': 0.04294898360967636, 'learning_rate': 3.804780876494024e-05, 'epoch': 0.72}\n",
      "{'loss': 0.1046, 'grad_norm': 0.029784811660647392, 'learning_rate': 3.7383798140770256e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0943, 'grad_norm': 0.017774412408471107, 'learning_rate': 3.671978751660027e-05, 'epoch': 0.8}\n",
      "{'loss': 0.1446, 'grad_norm': 0.28266409039497375, 'learning_rate': 3.6055776892430283e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0076, 'grad_norm': 0.05296073853969574, 'learning_rate': 3.53917662682603e-05, 'epoch': 0.88}\n",
      "{'loss': 0.315, 'grad_norm': 0.02228016033768654, 'learning_rate': 3.472775564409031e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4588, 'grad_norm': 93.40167236328125, 'learning_rate': 3.406374501992032e-05, 'epoch': 0.96}\n",
      "{'loss': 0.002, 'grad_norm': 0.021293289959430695, 'learning_rate': 3.339973439575033e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fff01977024ec2ab759f2598315f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06087331101298332, 'eval_accuracy': 0.9890219560878244, 'eval_runtime': 38.7775, 'eval_samples_per_second': 25.84, 'eval_steps_per_second': 6.473, 'epoch': 1.0}\n",
      "{'loss': 0.0016, 'grad_norm': 0.017108196392655373, 'learning_rate': 3.2735723771580345e-05, 'epoch': 1.04}\n",
      "{'loss': 0.1687, 'grad_norm': 0.01827259175479412, 'learning_rate': 3.207171314741036e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0079, 'grad_norm': 0.011581240221858025, 'learning_rate': 3.140770252324037e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0067, 'grad_norm': 0.014100435189902782, 'learning_rate': 3.0743691899070386e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0019, 'grad_norm': 6.783116340637207, 'learning_rate': 3.00796812749004e-05, 'epoch': 1.2}\n",
      "{'loss': 0.1118, 'grad_norm': 0.01173590961843729, 'learning_rate': 2.9415670650730414e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0022, 'grad_norm': 0.01238594576716423, 'learning_rate': 2.8751660026560427e-05, 'epoch': 1.27}\n",
      "{'loss': 0.188, 'grad_norm': 0.010920075699687004, 'learning_rate': 2.8087649402390438e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0018, 'grad_norm': 2.9739317893981934, 'learning_rate': 2.742363877822045e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0009, 'grad_norm': 0.010350622236728668, 'learning_rate': 2.6759628154050465e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0045, 'grad_norm': 0.014064068906009197, 'learning_rate': 2.609561752988048e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2158, 'grad_norm': 0.013564052060246468, 'learning_rate': 2.5431606905710493e-05, 'epoch': 1.47}\n",
      "{'loss': 0.232, 'grad_norm': 142.65972900390625, 'learning_rate': 2.4767596281540506e-05, 'epoch': 1.51}\n",
      "{'loss': 0.1801, 'grad_norm': 0.029837604612112045, 'learning_rate': 2.410358565737052e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0013, 'grad_norm': 0.044345106929540634, 'learning_rate': 2.3439575033200534e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0022, 'grad_norm': 0.013597630895674229, 'learning_rate': 2.2775564409030544e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0595, 'grad_norm': 164.12890625, 'learning_rate': 2.2111553784860558e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0009, 'grad_norm': 0.014809957705438137, 'learning_rate': 2.144754316069057e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0147, 'grad_norm': 0.012348607182502747, 'learning_rate': 2.0783532536520585e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0009, 'grad_norm': 0.04278215765953064, 'learning_rate': 2.01195219123506e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0008, 'grad_norm': 0.011843071319162846, 'learning_rate': 1.9455511288180613e-05, 'epoch': 1.83}\n",
      "{'loss': 0.001, 'grad_norm': 1.7251031398773193, 'learning_rate': 1.8791500664010626e-05, 'epoch': 1.87}\n",
      "{'loss': 0.1677, 'grad_norm': 0.009184150956571102, 'learning_rate': 1.812749003984064e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0007, 'grad_norm': 0.010410463437438011, 'learning_rate': 1.7463479415670654e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0007, 'grad_norm': 0.00814403872936964, 'learning_rate': 1.6799468791500664e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791771ce01ea4cee8893681143b59ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014515336602926254, 'eval_accuracy': 0.9970059880239521, 'eval_runtime': 38.8433, 'eval_samples_per_second': 25.796, 'eval_steps_per_second': 6.462, 'epoch': 2.0}\n",
      "{'loss': 0.0007, 'grad_norm': 0.014316472224891186, 'learning_rate': 1.6135458167330678e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0005, 'grad_norm': 0.009110601618885994, 'learning_rate': 1.547144754316069e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0639, 'grad_norm': 0.010127268731594086, 'learning_rate': 1.4807436918990705e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0005, 'grad_norm': 0.010387727990746498, 'learning_rate': 1.4143426294820719e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0005, 'grad_norm': 0.007823833264410496, 'learning_rate': 1.3479415670650731e-05, 'epoch': 2.19}\n",
      "{'loss': 0.09, 'grad_norm': 0.009702351875603199, 'learning_rate': 1.2815405046480745e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0034, 'grad_norm': 0.007953206077218056, 'learning_rate': 1.2151394422310758e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0005, 'grad_norm': 0.009092980995774269, 'learning_rate': 1.148738379814077e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0005, 'grad_norm': 0.006671322509646416, 'learning_rate': 1.0823373173970784e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0005, 'grad_norm': 0.009367533028125763, 'learning_rate': 1.0159362549800798e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0005, 'grad_norm': 0.009961668401956558, 'learning_rate': 9.49535192563081e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0008, 'grad_norm': 0.009512459859251976, 'learning_rate': 8.831341301460823e-06, 'epoch': 2.47}\n",
      "{'loss': 0.0004, 'grad_norm': 0.00590191874653101, 'learning_rate': 8.167330677290837e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0013, 'grad_norm': 6.133647441864014, 'learning_rate': 7.503320053120851e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0004, 'grad_norm': 0.00795028917491436, 'learning_rate': 6.839309428950863e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0729, 'grad_norm': 0.005582856014370918, 'learning_rate': 6.175298804780877e-06, 'epoch': 2.63}\n",
      "{'loss': 0.2003, 'grad_norm': 0.008684244938194752, 'learning_rate': 5.51128818061089e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0004, 'grad_norm': 0.006453663110733032, 'learning_rate': 4.847277556440903e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0005, 'grad_norm': 0.005852970760315657, 'learning_rate': 4.183266932270916e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0005, 'grad_norm': 0.007080550771206617, 'learning_rate': 3.51925630810093e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0004, 'grad_norm': 0.00638593127951026, 'learning_rate': 2.855245683930943e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0004, 'grad_norm': 0.007679908536374569, 'learning_rate': 2.1912350597609563e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0004, 'grad_norm': 0.005635757930576801, 'learning_rate': 1.5272244355909696e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0004, 'grad_norm': 0.009872226975858212, 'learning_rate': 8.632138114209828e-07, 'epoch': 2.95}\n",
      "{'loss': 0.0004, 'grad_norm': 0.009449142031371593, 'learning_rate': 1.99203187250996e-07, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557adabc15d24262852eb663e64692e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01121242344379425, 'eval_accuracy': 0.998003992015968, 'eval_runtime': 39.7749, 'eval_samples_per_second': 25.192, 'eval_steps_per_second': 6.311, 'epoch': 3.0}\n",
      "{'train_runtime': 565.3605, 'train_samples_per_second': 5.317, 'train_steps_per_second': 1.332, 'train_loss': 0.14054683096809523, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer, small_train_dataset, small_eval_dataset = fine_tune_llm(sentences, sentences, model_name=MODEL,num_labels=3, label_mapping = {'Negatief': 0, 'Neutraal': 1, 'Positief': 2}, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261e01979d3f49139b1cb2a2ab534f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred =  model_trained.predict(small_eval_dataset)\n",
    "\n",
    "label_mapping = {'Negatief': 0, 'Neutraal': 1, 'Positief': 2}\n",
    "\n",
    "mapped_labels = [label_mapping[label] for label in small_eval_dataset['Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset, small_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910179640718563\n",
      "0.9910179640718564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "# Calculate AUC\n",
    "f1 = f1_score(mapped_labels, np.argmax(y_pred.predictions, axis=1), average='micro')\n",
    "\n",
    "print(f1)\n",
    "accuracy_ = balanced_accuracy_score(mapped_labels, np.argmax(y_pred.predictions, axis=1))\n",
    "print(accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "small_train_dataset = dataset_train.map(tokenize_function, batched=True)\n",
    "small_eval_dataset = dataset_test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Apply label mapping\n",
    "small_train_dataset = small_train_dataset.map(map_labels, batched=True)\n",
    "small_eval_dataset = small_eval_dataset.map(map_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_labels(text, model):\n",
    "    encoded_input = tokenizer(text, padding=True,truncation=True,max_length=512, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    return config.id2label[ranking[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss, matthews_corrcoef, cohen_kappa_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming y_true and y_pred are already defined\n",
    "\n",
    "# Classification metrics\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "log_loss_value = log_loss(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "# Regression metrics (assuming y_true and y_pred are continuous values)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f'ROC-AUC Score: {roc_auc:.2f}')\n",
    "print(f'Log Loss: {log_loss_value:.2f}')\n",
    "print(f'Matthews Correlation Coefficient: {mcc:.2f}')\n",
    "print(f'Cohen\\'s Kappa: {cohen_kappa:.2f}')\n",
    "print(f'Mean Absolute Error: {mae:.2f}')\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences['prediction_KU'] = sentences['Sentence'].apply(sentiment_labels, model=model_trained.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['prediction_KU'] = sentences['prediction_KU'].replace(\"Positive\", \"Positief\")\n",
    "sentences['prediction_KU'] = sentences['prediction_KU'].replace(\"Negative\", \"Negatief\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
